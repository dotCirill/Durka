!!! Где
    Вирт, структуры данных 1989.

    Смотри 287 страницу.

    [На хабре](https://habr.com/en/post/114154/)

## Зачем

Бывают случаи, когда информация о вероятности появления ключа известна. В таких ситуациях характерно постоянство ключей, то есть ключи не добавляются, не удаляются, и сохраняется структура дерева. И для сокращения времени необходимо использование таких деревьев, в которых самые часто встречающиеся вершины находились бы ближе к корню дерева.

Изменилась метрика того, на сколько дерево хорошее -- раньше была средняя длина пути, теперь средне-взвешенная длина пути. Сумма всех вершин от корня, умноженная на вероятность.

Пример: $p_1 = \frac{1}{7}$, $p_2 = \frac{2}{7}$, $p_3 = \frac{4}{7}$. Самое оптимальное -- дерево (а), у него средре-взвешенная длина пути: $\frac{4\cdot 1}{7} + \frac{2\cdot 2}{7} + \frac{1\cdot 3}{7} = \frac{11}{7}$
![Деревья (Вирт рис.4.36)](Оптимального поиска/деревья.svg "Деревья (Вирт рис.4.36)")


## Основные положения

1. Псевдовершины -- некоторые поиски заканчиваем неудачно, создаем псевдовершины в квадратах.
2. Отойдем от вероятности -- считаем число поисков.
3. Длина средневзвешенного пути  $\sum_{i=1}^{n}a_ih_i + \sum_{j=1}^{m}b_jh'_j$.

Основное свойство: любое поддерево дерева оптимального поиска так же оптимально.
Так что фигачим снизу вверх, начиная с одной вершины. При добавлении $n+1$ элемента (в дерево из элементов) получаем $n+1$ различных вариантов. 

## Построение дерева

1. Построение таблицы весов $W_{ii}$ -- вес дерева $T_{ii}$ (дерево с ключами $k_i \ldots k_{i+1}$)
2. Достраиваем таблицу $W_{ij} = W_..........$ **TODO**